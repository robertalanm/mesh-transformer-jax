{
    "layers": 28,
    "d_model": 4096,
    "n_heads": 16,
    "n_vocab": 50400,
    "norm": "layernorm",
    "pe": "rotary",
    "pe_rotary_dims": 64,
  
    "seq": 2048,
    "cores_per_replica": 8,
    "per_replica_batch": 1,
    "gradient_accumulation_steps": 32,
  
    "warmup_steps": 347,
    "anneal_steps": 3126,
    "lr": 5.0e-5,
    "end_lr": 1.0e-5,
    "weight_decay": 0.1,
    "total_steps": 3473,
  
    "tpu_size": 8,
  
    "bucket": "gpt-sybil-bucket",
    "model_dir": "gpt-s-c2",
  
    "train_set": "c2.train.index",
    "val_set": {
    },
  
    "eval_harness_tasks": [
      "lambada",
      "piqa",
      "hellaswag",
      "winogrande",
      "mathqa",
      "pubmedqa"
    ],
  
    "val_batches": 100,
    "val_every": 500,
    "ckpt_every": 400,
    "keep_every": 4000,
  
    "name": "gpt-sybil-c2-trial_1",
    "wandb_project": "gpt-sybil",
    "comment": ""
  }
  